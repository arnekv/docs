---
title: Best practices for plotting charts with data points
description: A guide to using the Time Series API for high fidelity chart rendering in Cognite Data Fusion.
---

This best practice guide helps developers use the Time Series API to render precise, high-fidelity charts. While the focus is on high fidelity, you can adapt these techniques for lower granularity or lower fidelity use cases, such as weekly or monthly reports.

## Plot area resolution

To maximize data fidelity, determine the resolution of the plot area. If one pixel represents one data point, the horizontal resolution sets the maximum number of data points you can plot. This applies to both raw data points and aggregations. When the number of data points approaches the number of pixels, you achieve the highest data fidelity.

For example, with a 1080p monitor (1920 pixels wide) and a chart plot area of 1500 pixels, you can plot 1500 raw data points when fully zoomed in.

When you zoom out to view a longer period, you may have more data points than pixels. In this case, aggregate the data to fit the plot area.

Aggregation can happen in several ways. If the chart retrieves more raw data points than there are pixels, aggregation happens within the graphical layer, where the graphics driver aggregates or interpolates based on built-in algorithms. This client-side aggregation may render a reasonable approximation, but you lose control over the plot's behavior and may lose precision. It can also slow down client applications due to excessive data download and RAM usage.

A better approach is to use the aggregate function in the Time Series API. This lets you define the size of aggregation buckets and retrieve only the number of aggregated data points needed for the screen, improving performance and reducing resource usage.

In summary, when the number of data points exceeds the number of pixels, use the Time Series API aggregate function.

## Average, min, max, and standard variance

When plotting aggregated charts, calculating the average value for each bucket is a natural first step. However, if there are outliers, using only the average may lose valuable information. Consider also plotting min and max values with the average for a fuller picture.

You can gain further insight by including variance aggregates. For example, the square root of the **continuousVariance** value for a period gives the standard deviation, which you can plot to show the proportion of outliers from the mean.

<Note>
For most time series analysis, use **continuousVariance**. For fixed-frequency time series, use **discreteVariance**.
</Note>

When maximizing screen real estate for high fidelity (aggregated data points per pixel), use the format shown in the chart below.

<img className="screenshot" src="/images/cdf/dev/concepts/resource_types/high_fidelity_datapoints_avg_min_max.png" alt="High fidelity plot with average, min and max values " width="100%"/>

For less granular plots with fewer aggregate buckets, a scatter plot format may be more appropriate:

<img className="screenshot" src="/images/cdf/dev/concepts/resource_types/LoFi_daily_avg_min_max.png" alt="Daily granularity plot with average, min and max values " width="100%"/>

## How to choose aggregate bucket size

Let's use an example to illustrate how to select the best aggregate bucket size with the Time Series API. This example uses the average aggregation, but the approach also works for min and max aggregates.

<Note>
This example does not suggest that plotting the highest fidelity graph is best practice in all cases. Choose the visualization mode that fits the customer use case. In some scenarios, a lower granularity, scatter plot may be more useful than the high granularity example here.
</Note>

Example 1:
- A sensor produces temperature readings at 1 Hz (60 data points per minute).
- You want to plot the readings on a chart with a horizontal resolution of 1500 px.
- The initial timespan is 30 days.

In this scenario, there are 2,592,000 raw data points, but only 1500 pixels to plot them. You need to aggregate the data into buckets.

The X-axis represents time in days. Each pixel can represent 28.8 minutes (total minutes in plot: 43,200 / 1500). Round each aggregate bucket up to 30 minutes. Rounding up ensures the number of buckets is less than the number of pixels.

With each pixel containing a 30-minute aggregation, there are 1440 buckets in 30 days, so you can plot the data within the plot area.

The Time Series API supports granularities in days, hours, minutes, or seconds. In this example, an API call might look like this:

```json
{
  "items": [
    {
      "start": 2023-04-01T00:00:00,
      "end": 2023-04-30T23:59:59,
      "limit": 0,
      "aggregates": [
        "average"
      ],
      "granularity": "30m",
      "targetUnit": "temperature:deg_f",
      "targetUnitSystem": "imperial",
      "includeOutsidePoints": false,
      "cursor": "string",
      "id": 1
    }
  ],
  "start": 0,
  "end": 0,
  "limit": 100,
  "aggregates": [
    "average"
  ],
  "granularity": "30m",
  "includeOutsidePoints": false,
  "ignoreUnknownIds": false
}
```

### Changing the plot time span

If you want to zoom out and look at a calendar quarter (April 1 to June 30), the quarter has 91 days (131,040 minutes).

Divide the minutes by the number of pixels to get an aggregate bucket size of 87.36 minutes. Round up to 90 minutes. This results in 1456 buckets, just within the pixel count.

### Changes in plot resolution

Modern web applications may be resized for different screens and resolutions. Display resolutions may range from 1024px (older laptops) to 7680px (8K screens). Video walls can be even larger. Users may also resize browser windows, changing the plot area resolution. In these cases, calculate aggregate bucket sizes where the plot area pixel count is a variable.

# Summary

Calculate the appropriate aggregate bucket size by considering:

- The frequency of the time series data points
- The desired time span to visualize
- The horizontal resolution of the plot area
- The use case requirements for high vs. low fidelity (managing information overload)

Round up the aggregate bucket size to the nearest sensible number to ensure the data fits within the plot area.